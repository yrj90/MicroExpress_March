test_net: "Experiments/20170314_transition(1)_batchSize(100)_trainingEpoches(100)_/1/net_valid.prototxt"
test_iter: 0
test_interval: 500000
base_lr: 0.00999999977648
display: 10000000
max_iter: 50000
lr_policy: "step"
gamma: 0.10000000149
momentum: 0.899999976158
weight_decay: 0.00499999988824
stepsize: 10000
snapshot: 50000000
snapshot_prefix: ".//snapshot_lstm_lip_reading_fold_1"
solver_mode: GPU
random_seed: 1701
net: "Experiments/20170314_transition(1)_batchSize(100)_trainingEpoches(100)_/1/net_train.prototxt"
average_loss: 1000
clip_gradients: 5.0
